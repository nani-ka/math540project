# The Project for MATH 540 Statistical Learning.

Non-linear activation functions are crucial for the expressive powers of multi-layer perceptrons. One common practice in deep learning is applying the same activation function for all neurons of the same hidden layer. However, one might ask ``What happens if different activation functions are mixed within a single hidden layer?''. This study attempts to answer this question using both theoretical and empirical analysis. The results of this study suggest that baseline models that use the same activation function perform better than modified models that use different activation functions within a single hidden layer. However, applying batch normalization helps to improve the performance of the modified models and makes it comparable to the performance of baseline models. The conclusion is that mixing different activation layers within a single hidden layer does not benefit and may even worsen the model performance.
